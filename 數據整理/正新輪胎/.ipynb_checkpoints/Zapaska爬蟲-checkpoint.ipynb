{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import json\n",
    "from lxml import etree\n",
    "import time\n",
    "import random\n",
    "import tkinter as tk\n",
    "from tkinter import filedialog\n",
    "import re\n",
    "pd.set_option('display.max_rows', 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "re.search('[A-Za-zА-Яа-я]+', jj).group() # 匹配品牌名\n",
    "'(R\\d+\\W\\d)|(R\\d+\\w)|(R\\d+)' # 匹配輪徑"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_urls():\n",
    "    url = 'https://tyres.spb.ru/catalog_tires_level_search_d_0_w_0_h_0_page_1'\n",
    "    headers = {'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 11_2_3) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/89.0.4389.90 Safari/537.36',\n",
    "              'Host': 'tyres.spb.ru', 'Connection': 'keep-alive', 'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9', \n",
    "              'Referer': 'https://www.google.com', 'Accept-Encoding': 'gzip, deflate, br', \n",
    "               'Accept-Language': 'zh-TW,zh;q=0.9,ru;q=0.8,en-US;q=0.7,en;q=0.6,ja;q=0.5', \n",
    "              'Upgrade-Insecure-Requests': '1'}\n",
    "    res = requests.get(url, headers=headers)\n",
    "    soup = bs(res.content, 'html.parser')\n",
    "    body = soup.find('ul', class_='pagination justify-content-center')\n",
    "    pages = []\n",
    "    for b in body:\n",
    "        try:\n",
    "    #         print(b.text)\n",
    "            if b.text != '':\n",
    "                pages.append(b.text)\n",
    "        except AttributeError:\n",
    "            pass\n",
    "    links = []\n",
    "    for i in range(1, int(pages[-1])+1):\n",
    "        url = 'https://tyres.spb.ru/catalog_tires_level_search_d_0_w_0_h_0_page_' + str(i)\n",
    "        res = requests.get(url, headers=headers)\n",
    "        soup = bs(res.content, 'html.parser')\n",
    "        hrefs = soup.find_all('div', class_='wrapper-body')\n",
    "        for href in hrefs:\n",
    "            link = href.find('a', class_='card-title card-title-good')['href']\n",
    "            product_page = 'https://tyres.spb.ru/'\n",
    "            redirect = product_page + link\n",
    "            if redirect not in links:\n",
    "                links.append(redirect)\n",
    "            else:\n",
    "                pass\n",
    "        time.sleep(2)\n",
    "    return links\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "links = get_urls()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def parse(url):\n",
    "    headers = {'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 11_2_3) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/89.0.4389.90 Safari/537.36',\n",
    "              'Host': 'tyres.spb.ru', 'Connection': 'keep-alive', 'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9', \n",
    "              'Referer': 'https://www.google.com', 'Accept-Encoding': 'gzip, deflate, br', \n",
    "               'Accept-Language': 'zh-TW,zh;q=0.9,ru;q=0.8,en-US;q=0.7,en;q=0.6,ja;q=0.5', \n",
    "              'Upgrade-Insecure-Requests': '1'}\n",
    "    res = requests.get(url, headers=headers)\n",
    "    soup = bs(res.content, 'html.parser')\n",
    "    brand = soup.find('div', {'class': 'comments-list', 'data-id': 'tires'})['data-brand']\n",
    "    model = soup.find('div', {'class': 'comments-list', 'data-id': 'tires'})['data-model']\n",
    "    price = soup.find('div', {'id': 'card-p', 'class': 'price card'})['data-price']\n",
    "    body = soup.find_all('p', class_='type')\n",
    "    attr_dict = {}\n",
    "    for b in body:\n",
    "        item_list = b.text.split(':')\n",
    "        attr_dict.setdefault(item_list[0], item_list[1])\n",
    "    \n",
    "    data = {'Brand': brand, 'Model': model, 'Price': price, 'Season': attr_dict['Сезонность'],\n",
    "                   'Type': attr_dict['Тип автомобиля'], 'Wide': attr_dict['Ширина профиля'],\n",
    "                   'Profile': attr_dict['Высота профиля'], 'Diameter': attr_dict['Диаметр'],\n",
    "                   'Speed': attr_dict['Индекс скорости'], 'Loading': attr_dict['Индекс нагрузки'],\n",
    "                   'Studded': attr_dict['Шипы']}\n",
    "    return data\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import http.client\n",
    "http.client.HTTPConnection._http_vsn = 10\n",
    "http.client.HTTPConnection._http_vsn_str = 'HTTP/1.0'\n",
    "\n",
    "columns = ['Brand', 'Model', 'Price', 'Season', 'Type', 'Wide', 'Profile', 'Diameter', 'Speed', 'Loading', 'Studded']\n",
    "df = pd.DataFrame(columns=columns)\n",
    "print('共有{}筆需要爬取。'.format(len(links)))\n",
    "n = 0\n",
    "for url in links:\n",
    "#     print(url)\n",
    "    data = parse(url)\n",
    "    df = df.append(data, ignore_index=True)\n",
    "    n += 1\n",
    "    print('第{}筆抓取完畢'.format(n))\n",
    "df.shape\n",
    "df.to_excel('/Users/kai/Desktop/exclusive_tires.xlsx', index=False)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import concurrent.futures\n",
    "import http.client\n",
    "http.client.HTTPConnection._http_vsn = 10\n",
    "http.client.HTTPConnection._http_vsn_str = 'HTTP/1.0'\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "columns = ['Brand', 'Model', 'Price', 'Season', 'Type', 'Wide', 'Profile', 'Diameter', 'Speed', 'Loading', 'Studded']\n",
    "df = pd.DataFrame(columns=columns)\n",
    "\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=30) as executor:\n",
    "    obj = list(executor.map(parse, links))\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "totla_time = end_time - start_time\n",
    "print('總共花了{}秒完成爬蟲'.format(totla_time))\n",
    "    \n",
    "for d in obj:\n",
    "    df = df.append(d, ignore_index=True)\n",
    "\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[(df['Price'] != '')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Price'] = df['Price'].astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Wide'] = df['Wide'].str.replace('мм.', '').str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Profile'] = df['Profile'].str.replace('%', '').str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Diameter'] = df['Diameter'].str.replace('\"', '').str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type_keys = df['Type'].unique().tolist()\n",
    "type_values = ['PCR', 'SUV', 'LTR', '4x4', 'N/A']\n",
    "type_dict = dict(zip(type_keys, type_values))\n",
    "df['Type'] = df['Type'].map(type_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "season_keys = df['Season'].unique().tolist()\n",
    "season_values = ['Winter', 'Summer', 'All-season']\n",
    "season_dict = dict(zip(season_keys, season_values))\n",
    "df['Season'] = df['Season'].map(season_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_excel('/Users/kai/Desktop/exclusive_tires.xlsx', index=False)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://tyres.spb.ru/catalog_tires_level_search_d_0_w_0_h_0_page_2'\n",
    "headers = {'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 11_2_3) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/89.0.4389.90 Safari/537.36'}\n",
    "\n",
    "res = requests.get(url, headers=headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "soup = bs(res.content, 'html.parser')\n",
    "hrefs = soup.find_all('div', class_='wrapper-body')\n",
    "for href in hrefs:\n",
    "    link = href.find('a', class_='card-title card-title-good')['href']\n",
    "    product_page = 'https://tyres.spb.ru/'\n",
    "    redirect = product_page + link\n",
    "    print(redirect)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "soup = bs(res.content, 'html.parser')\n",
    "brand = soup.find('div', {'class': 'comments-list', 'data-id': 'tires'})['data-brand']\n",
    "model = soup.find('div', {'class': 'comments-list', 'data-id': 'tires'})['data-model']\n",
    "price = soup.find('div', {'id': 'card-p', 'class': 'price card'})['data-price']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['Brand', 'Model', 'Price', 'Season', 'Type', 'Wide', 'Profile', 'Diameter', 'Speed', 'Loading', 'Studded']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "body = soup.find_all('p', class_='type')\n",
    "attr_dict = {}\n",
    "for b in body:\n",
    "    item_list = b.text.split(':')\n",
    "    attr_dict.setdefault(item_list[0], item_list[1])\n",
    "    \n",
    "attr_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exclusive爬蟲"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://tyres.spb.ru/catalog_tires_level_search_d_0_w_0_h_0_page_1'\n",
    "headers = {'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 11_2_3) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/89.0.4389.90 Safari/537.36', \n",
    "          'Referer': 'https://tyres.spb.ru/catalog_tires_level_search_d_0_w_0_h_0_page_2'}\n",
    "res = requests.get(url, headers=headers)\n",
    "soup = bs(res.content, 'html.parser')\n",
    "body = soup.find('ul', class_='pagination justify-content-center')\n",
    "pages = []\n",
    "for b in body:\n",
    "    try:\n",
    "#         print(b.text)\n",
    "        if b.text != '':\n",
    "            pages.append(b.text)\n",
    "    except AttributeError:\n",
    "        pass\n",
    "\n",
    "columns = ['Brand', 'Model', 'Price', 'Season', 'Type', 'Wide', 'Profile', 'Diameter', 'Speed', 'Loading', 'Studded']\n",
    "df = pd.DataFrame(columns=columns)\n",
    "\n",
    "for i in range(1, int(pages[-1])+1):\n",
    "    url = 'https://tyres.spb.ru/catalog_tires_level_search_d_0_w_0_h_0_page_' + str(i)\n",
    "    soup = bs(res.content, 'html.parser')\n",
    "    hrefs = soup.find_all('div', class_='wrapper-body')\n",
    "    for href in hrefs:\n",
    "        link = href.find('a', class_='card-title card-title-good')['href']\n",
    "        product_page = 'https://tyres.spb.ru/'\n",
    "        redirect = product_page + link\n",
    "        print(redirect)\n",
    "        res = requests.get(redirect, headers=headers)\n",
    "        soup = bs(res.content, 'html.parser')\n",
    "        brand = soup.find('div', {'class': 'comments-list', 'data-id': 'tires'})['data-brand']\n",
    "        model = soup.find('div', {'class': 'comments-list', 'data-id': 'tires'})['data-model']\n",
    "        price = soup.find('div', {'id': 'card-p', 'class': 'price card'})['data-price']\n",
    "        body = soup.find_all('p', class_='type')\n",
    "        attr_dict = {}\n",
    "        for b in body:\n",
    "            item_list = b.text.split(':')\n",
    "            attr_dict.setdefault(item_list[0], item_list[1])\n",
    "        print({'Brand': brand, 'Model': model, 'Price': price, 'Season': attr_dict['Сезонность'],\n",
    "               'Type': attr_dict['Тип автомобиля'], 'Wide': attr_dict['Ширина профиля'],\n",
    "               'Profile': attr_dict['Высота профиля'], 'Diameter': attr_dict['Диаметр'],\n",
    "               'Speed': attr_dict['Индекс скорости'], 'Loading': attr_dict['Индекс нагрузки'],\n",
    "               'Studded': attr_dict['Шипы']})\n",
    "        df = df.append({'Brand': brand, 'Model': model, 'Price': price, 'Season': attr_dict['Сезонность'],\n",
    "                   'Type': attr_dict['Тип автомобиля'], 'Wide': attr_dict['Ширина профиля'],\n",
    "                   'Profile': attr_dict['Высота профиля'], 'Diameter': attr_dict['Диаметр'],\n",
    "                   'Speed': attr_dict['Индекс скорости'], 'Loading': attr_dict['Индекс нагрузки'],\n",
    "                   'Studded': attr_dict['Шипы']}, ignore_index=True)\n",
    "    print('爬完第{}頁'.format(str(i)))\n",
    "df.to_excel('/Users/kai/Desktop/exclusive_tires.xlsx', index=False)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 正片區"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "columns = ['Name', 'Price', 'Model', 'Size', 'Sea_type', 'Index']\n",
    "df = pd.DataFrame(columns=columns)\n",
    "\n",
    "url = 'https://www.sibzapaska.ru/include/ajax/shini.php'\n",
    "headers = {'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 11_2_3) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/89.0.4389.90 Safari/537.36'}\n",
    "\n",
    "page_n = 1\n",
    "while True:\n",
    "    data = {'iblock': 17, 'PAGEN_1': page_n, 'kol': 24, 'by1': 'DESC', 'or1': 'ASC', 'by2': 'CATALOG_PRICE_1',\n",
    "            'or2': 'ASC', 'min': 'undefined', 'max': 'undefined'}\n",
    "    res = requests.post(url, headers=headers, data=data)\n",
    "    body = bs(res.content, 'html.parser')\n",
    "    for d in body.findAll('div', class_='col-lg-3 col-md-4 col-sm-6 tire-list__col'):\n",
    "        href = d.find('a', class_='item__name')['href']\n",
    "        home = 'https://www.sibzapaska.ru'\n",
    "        product = home + href\n",
    "        res = requests.get(product, headers=headers)\n",
    "        content = bs(res.content, 'html.parser')\n",
    "        brand = content.find('h1').text\n",
    "        price = content.find('span', class_='detail__total-price').text\n",
    "        properties = content.find_all('span', class_='property__item')\n",
    "        if len(properties) == 3:\n",
    "            for p in properties:\n",
    "                if 'Протектор' not in p.text:\n",
    "                    model = properties[0].text\n",
    "                    size = properties[1].text\n",
    "                    index = properties[2].text\n",
    "                    sea_type = 'N/A'\n",
    "                elif 'Скоростной индекс' not in p.text:\n",
    "                    model = properties[0].text\n",
    "                    size = properties[1].text\n",
    "                    sea_type = properties[2].text\n",
    "                    index = 'N/A'\n",
    "                elif 'Размер' not in p.text:\n",
    "                    model = properties[0].text\n",
    "                    size = 'N/A'\n",
    "                    sea_type = properties[1].text\n",
    "                    index = properties[2].text\n",
    "                elif 'Модель' not in p.text:\n",
    "                    model = 'N.A'\n",
    "                    size = properties[0].text\n",
    "                    sea_type = properties[1].text\n",
    "                    index = properties[2].text\n",
    "                else:\n",
    "                    model = 'N/A'\n",
    "                    size = properties[0].text\n",
    "                    try:\n",
    "                        sea_type = properties[1].text\n",
    "                    except IndexError:\n",
    "                        sea_type = 'N/A'\n",
    "                        pass\n",
    "                    try:\n",
    "                        index = properties[2].text\n",
    "                    except IndexError:\n",
    "                        index = 'N/A'\n",
    "                        pass\n",
    "        elif len(properties) == 2:\n",
    "            model = 'N/A'\n",
    "            size = properties[0].text\n",
    "            sea_type = properties[1].text\n",
    "            index = 'N/A'\n",
    "        else: \n",
    "            model = properties[0].text\n",
    "            size = properties[1].text\n",
    "            try:\n",
    "                sea_type = properties[2].text\n",
    "            except IndexError:\n",
    "                sea_type = 'N/A'\n",
    "                pass\n",
    "            try:\n",
    "                index = properties[3].text\n",
    "            except IndexError:\n",
    "                index = 'N/A'\n",
    "                pass\n",
    "        df = df.append({'Name': brand, 'Price': price, 'Model': model, 'Size': size, 'Sea_type': sea_type, 'Index': index}, ignore_index=True)\n",
    "    print(df)\n",
    "    \n",
    "    verify = []\n",
    "    pages = body.find('div', class_='navi navi_ajax')\n",
    "    for page in pages:\n",
    "        try:\n",
    "            verify.append(page.text)\n",
    "        except AttributeError:\n",
    "            pass\n",
    "    \n",
    "    if '>' in verify:\n",
    "        page_n += 1\n",
    "#         time.sleep(random.randint(0,1))\n",
    "    else:\n",
    "        print('爬取完畢，共{}頁'.format(page_n))\n",
    "        break\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_excel('/Users/kai/Desktop/zapaska_tires.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ETL過程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel('/Users/kai/Desktop/zapaska_tires.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Name'] = df['Name'].str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Model'].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = df[~(df['Name'] == '') & (~df['Model'].isna())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Brand'] = df['Name'].str.extract(r'([A-Za-zА-Яа-я]+)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rim = df['Size'].str.extract(r'(R\\d+\\W\\d)|(R\\d+\\w)|(R\\d+)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rim[0] = rim[0].fillna('NO')\n",
    "rim[1] = rim[1].fillna('NO')\n",
    "df['Diameter'] = rim.apply(lambda x: x[0] if x[0] != 'NO' else x[1] if x[1] != 'NO' else 'No', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Size'] = df['Size'].str.replace('Размер: ', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Model'] = df['Model'].str.replace('Модель: ', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Sea_type'] = df['Sea_type'].str.replace('Протектор: ', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "types = df['Sea_type'].str.split(',', expand=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "types.columns = ['Season', 'Type', 'Type2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Index'] = df['Index'].str.replace('Скоростной индекс: ', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Final_price'] = df['Price'].str.replace('руб.', '').str.split('.', expand=True)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([df, types], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Type'] = df['Type'] + df['Type2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Speed'] = df['Index'].str.extract(r'([A-Z])')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Wide'] = df['Size'].str.extract(r'(\\d+)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Profile'] = df['Size'].str.extract(r'(\\d+R)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Profile'] = df['Profile'].str.replace('R', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df['Loading'] = df['Index'].str.strip().map(lambda x: x[:-1] if pd.notna(x) else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[['Name', 'Brand', 'Model', 'Final_price', 'Size', 'Wide', 'Profile', 'Loading', 'Profile', 'Diameter', 'Season', 'Type']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "season_keys = df['Season'].unique().tolist()\n",
    "season_values = ['Summer', 'Studded', 'Non-Sudded', 'LTR', 'N/A']\n",
    "season_dict = dict(zip(season_keys, season_values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Season'] = df['Season'].map(season_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type_keys = df['Type'].unique().tolist()\n",
    "type_values = ['N/A', ' LTR', 'Off-road']\n",
    "type_dict = dict(zip(type_keys, type_values))\n",
    "df['Type'] = df['Type'].map(type_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Final_price'] = df['Final_price'].map(lambda x: '0' if x == ' ' else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df['Final_price'] = df['Final_price'].astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_excel('/Users/kai/Desktop/zapaska_tires_etl.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 畫圖"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zscore = preprocessing.StandardScaler()\n",
    "price = zscore.fit_transform(df[['Final_price']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Price'] = price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.box(df, x='Brand', y='Price', color='Season').update_xaxes(categoryorder='total descending')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "root = tk.Tk()\n",
    "print('請選擇要儲存到哪裡？\\n')\n",
    "root.withdraw()\n",
    "path = filedialog.askdirectory()\n",
    "file_name = input('請輸入檔案名稱：\\n')\n",
    "full_name = path + '/' + file_name + '.xlsx'\n",
    "etl.to_excel(full_name, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 不好的方式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['Name', 'Price']\n",
    "df = pd.DataFrame(columns=columns)\n",
    "\n",
    "url = 'https://www.sibzapaska.ru/include/ajax/shini.php'\n",
    "headers = {'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 11_2_3) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/89.0.4389.90 Safari/537.36'}\n",
    "\n",
    "page_n = 1\n",
    "while True:\n",
    "    data = {'iblock': 17, 'PAGEN_1': page_n, 'kol': 24, 'by1': 'DESC', 'or1': 'ASC', 'by2': 'CATALOG_PRICE_1',\n",
    "        'or2': 'ASC', 'min': 'undefined', 'max': 'undefined'}\n",
    "    \n",
    "    res = requests.post(url, headers=headers, data=data)\n",
    "#     print(res.status_code)\n",
    "    body = bs(res.content, 'html.parser')\n",
    "    for d in body.findAll('div', class_='col-lg-3 col-md-4 col-sm-6 tire-list__col'):\n",
    "        name = d.find('a', class_='item__name').text\n",
    "        price = d.find('span', class_='item__price').text\n",
    "#         print(name, price)\n",
    "        df = df.append({'Name': name, 'Price': price}, ignore_index=True)\n",
    "    print(df, '\\n已經爬取{}頁'.format(page_n))\n",
    "    \n",
    "    verify = []\n",
    "    pages = body.find('div', class_='navi navi_ajax')\n",
    "    for page in pages:\n",
    "        try:\n",
    "            verify.append(page.text)\n",
    "        except AttributeError:\n",
    "            pass\n",
    "    \n",
    "    if '>' in verify:\n",
    "        page_n += 1\n",
    "        time.sleep(random.randint(0,1))\n",
    "    else:\n",
    "        print('爬取完畢，共{}頁'.format(page_n))\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 非同步請求的方式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_urls():\n",
    "    urls = []\n",
    "    url = 'https://www.sibzapaska.ru/include/ajax/shini.php'\n",
    "    headers = {'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 11_2_3) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/89.0.4389.90 Safari/537.36'}\n",
    "\n",
    "    page_n = 1\n",
    "    while True:\n",
    "        data = {'iblock': 17, 'PAGEN_1': page_n, 'kol': 24, 'by1': 'DESC', 'or1': 'ASC', 'by2': 'CATALOG_PRICE_1',\n",
    "            'or2': 'ASC', 'min': 'undefined', 'max': 'undefined'}\n",
    "        res = requests.post(url, headers=headers, data=data)\n",
    "        body = bs(res.content, 'html.parser')\n",
    "        for d in body.findAll('div', class_='col-lg-3 col-md-4 col-sm-6 tire-list__col'):\n",
    "            href = d.find('a', class_='item__name')['href']\n",
    "            home = 'https://www.sibzapaska.ru'\n",
    "            product = home + href\n",
    "            urls.append(product)\n",
    "        verify = []\n",
    "        pages = body.find('div', class_='navi navi_ajax')\n",
    "        for page in pages:\n",
    "            try:\n",
    "                verify.append(page.text)\n",
    "            except AttributeError:\n",
    "                pass\n",
    "\n",
    "        if '>' in verify:\n",
    "            page_n += 1\n",
    "    #         time.sleep(random.randint(0,1))\n",
    "        else:\n",
    "            print('爬取完畢，共{}頁'.format(page_n))\n",
    "            return urls\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "urls = get_urls()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['Name', 'Price', 'Model', 'Size', 'Sea_type', 'Index']\n",
    "df = pd.DataFrame(columns=columns)\n",
    "\n",
    "url = 'https://www.sibzapaska.ru/include/ajax/shini.php'\n",
    "headers = {'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 11_2_3) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/89.0.4389.90 Safari/537.36'}\n",
    "\n",
    "page_n = 1\n",
    "while True:\n",
    "    urls = []\n",
    "    data = {'iblock': 17, 'PAGEN_1': page_n, 'kol': 24, 'by1': 'DESC', 'or1': 'ASC', 'by2': 'CATALOG_PRICE_1',\n",
    "            'or2': 'ASC', 'min': 'undefined', 'max': 'undefined'}\n",
    "    res = requests.post(url, headers=headers, data=data)\n",
    "    body = bs(res.content, 'html.parser')\n",
    "    for d in body.findAll('div', class_='col-lg-3 col-md-4 col-sm-6 tire-list__col'):\n",
    "        href = d.find('a', class_='item__name')['href']\n",
    "        home = 'https://www.sibzapaska.ru'\n",
    "        product = home + href\n",
    "        urls.append(product)\n",
    "    reqs = [grequests.get(link, headers=headers) for link in urls]\n",
    "    reps = grequests.map(reqs)\n",
    "    \n",
    "    for res in reps:\n",
    "        content = bs(res.content, 'html.parser')\n",
    "        brand = content.find('h1').text\n",
    "        price = content.find('span', class_='detail__total-price').text\n",
    "        properties = content.find_all('span', class_='property__item')\n",
    "        model = properties[0].text\n",
    "        size = properties[1].text\n",
    "        sea_type = properties[2].text\n",
    "        try:\n",
    "            index = properties[3].text\n",
    "        except IndexError:\n",
    "            index = 'N/A'\n",
    "            pass\n",
    "        df = df.append({'Name': brand, 'Price': price, 'Model': model, 'Size': size, 'Sea_type': sea_type, 'Index': index}, ignore_index=True)\n",
    "    print(df)\n",
    "    \n",
    "    verify = []\n",
    "    pages = body.find('div', class_='navi navi_ajax')\n",
    "    for page in pages:\n",
    "        try:\n",
    "            verify.append(page.text)\n",
    "        except AttributeError:\n",
    "            pass\n",
    "    \n",
    "    if '>' in verify:\n",
    "        page_n += 1\n",
    "#         time.sleep(random.randint(0,1))\n",
    "    else:\n",
    "        print('爬取完畢，共{}頁'.format(page_n))\n",
    "        break\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 數據清洗"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "etl = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "price = etl['Price'].str.split(' ', expand=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "price = price[1].map(lambda x: 0 if x == '' else x.replace('.00', ''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "etl['Price'] = price.astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "etl['Name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "etl['Name'].str.replace('Шина', '').str.extract(r'(\\d+\\,?\\d*\\/\\d+)', expand=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
